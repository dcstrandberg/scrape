#import time
#TODO: Add exception handling for timeouts, Add some form of analysis?, Add Date and Search Term fields to CSV
#I made an infinite loop somehow -- the CSV files show that I made lots of page entries

from proxy_list_scrape import scrapeProxyList
from datetime import date
import random
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from multiprocessing import Process, Queue, Pool, Manager
import threading
import sys

# use proxies in requests so as to proxy your request via a proxy server
# as some sites may block the IP if traffic generated by an IP is too high
proxies = scrapeProxyList() 
proxyCounter = 0
startTime = time.time()
qcount = 0
searchTerms=[] #list to store keyword of product
dates = [] #list to store date of search for product
products=[] #List to store name of the product
prices=[] #List to store price of the product
pages=[] #List to store ratings of the product
no_pages = 4 #20
keywords = ['soda','cereal', 'bars', 'drinks', 'water', 'coke']


def get_data(keyword, pageNo,q):  
    #use this shit to use the global proxyCounter variable
    global proxyCounter
    
    #wait for a random period of time before fetching the next page, to help avoid being blocked by amazon
    time.sleep(random.random()) 
    
    headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0", "Accept-Encoding":"gzip, deflate", "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "DNT":"1","Connection":"close", "Upgrade-Insecure-Requests":"1"}
    
    #This is where I need to add an exception watch -- because this is where the timeout happens
    #Keep trying until an exception isn't raised
    noError = False
    
    while not noError:
        try:
            r = requests.get("https://www.amazon.com/s?k=" + keyword + "!&page=" + str(pageNo), headers=headers, proxies=proxies[proxyCounter % len(proxies)])
        except:
            print("There was a connection error")
            proxyCounter += 1
        else:
            print("Now things are ok")
            noError = True

    content = r.content
    soup = BeautifulSoup(content, features="lxml")
    #print(soup.encode('utf-8')) # uncomment this in case there is some non UTF-8 character in the content and
                                 # you get error
	
    for d in soup.findAll('div', attrs={'class':'sg-col-4-of-12 sg-col-8-of-16 sg-col-16-of-24 sg-col-12-of-20 sg-col-24-of-32 sg-col sg-col-28-of-36 sg-col-20-of-28'}):
        name = d.find('span', attrs={'class':'a-size-medium a-color-base a-text-normal'})
        price = d.find('span', attrs={'class':'a-offscreen'})
        #rating = d.find('span', attrs={'class':'a-icon-alt'})
        all=[]
		
        all.append(keyword)
        all.append(date.today())

        if name is not None:
            all.append(name.text)
        else:
            all.append("unknown-product")
 
        if price is not None:
            all.append(price.text)
        else:
            all.append('$0')
		
        all.append(str(pageNo))

        #if rating is not None:
            #all.append(rating.text)
        #else:
            #all.append('-1')
        q.put(all)
        #print("---------------------------------------------------------------") 
results = []
if __name__ == "__main__":
    m = Manager()
    q = m.Queue() # use this manager Queue instead of multiprocessing Queue as that causes error
    
    


    #Adding a for loop to cycle through the keywords -- not sure if a queue can work inside a for loop
    for word in keywords:
        p = {}    
        if sys.argv[1] in ['t', 'p']: # user decides which method to invoke: thread, process or pool
            for i in range(1,no_pages):
                if sys.argv[1] in ['t']:
                    print("starting thread: ",proxyCounter)
                    #try: 
                    p[i] = threading.Thread(target=get_data, args=(word, i,q))
                    p[i].start()

                    #increment ProxyCounter
                    proxyCounter += 1

                    #except:
                        #print("Proxy Index was " + proxyCounter)
                        #proxyCounter += 1
                        #print("Proxy Index is now " + proxyCounter)

                        #print("Loop Index was " + i)
                        #i -= 1
                        #rint("Loop Index is now " + i)

                elif sys.argv[1] in ['p']:
                    print("starting process: ",proxyCounter)
                    #try:
                    p[i] = Process(target=get_data, args=(word, i,q))
                    p[i].start()

                    #increment ProxyCounter
                    proxyCounter += 1

                    #except:
                        #print("Proxy Index was " + proxyCounter)
                        #proxyCounter += 1
                        #print("Proxy Index is now " + proxyCounter)

                        #print("Loop Index was " + i)
                        #i -= 1
                        #print("Loop Index is now " + i)

            # join should be done in seperate for loop 
            # reason being that once we join within previous for loop, join for p1 will start working
            # and hence will not allow the code to run after one iteration till that join is complete, ie.
            # the thread which is started as p1 is completed, so it essentially becomes a serial work instead of 
            # parallel
            for i in range(1,no_pages):
                p[i].join()
        while q.empty() is not True:
            qcount = qcount+1
            queue_top = q.get()
            searchTerms.append(queue_top[0])
            dates.append(queue_top[1])
            products.append(queue_top[2])
            prices.append(queue_top[3])
            pages.append(queue_top[4])

    #Only run once everything is done        
    print("total time taken: ", str(time.time()-startTime), " qcount: ", qcount)
    #print(q.get())
    df = pd.DataFrame({'Keyword':searchTerms,'Date':dates, 'Product Name':products, 'Price':prices, 'Page':pages})
    #print(df)
    df.to_csv('./amazon_data/' + str(date.today()) + '-SearchList.csv', index=False, encoding='utf-8')
